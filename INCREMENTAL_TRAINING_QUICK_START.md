# ğŸš€ Incremental Training - Quick Start

## What You Asked For
> "I want fine-tuning to build on previous training, not start from scratch each time"

## What You Got âœ…

**TRUE INCREMENTAL TRAINING!** Each training run now builds on the previous one.

---

## How It Works (Simple Version)

### Before:
```
Run 1: Base â†’ Train â†’ v1 âŒ Discarded
Run 2: Base â†’ Train â†’ v2 âŒ Discarded  
Run 3: Base â†’ Train â†’ v3 âœ… Only this one matters
```

### Now:
```
Run 1: Base â†’ Train â†’ v1 âœ… Saved
Run 2: v1 â†’ Train â†’ v2   âœ… Builds on v1
Run 3: v2 â†’ Train â†’ v3   âœ… Builds on v2
```

**Result**: v3 contains improvements from v1 + v2 + v3! ğŸ‰

---

## How To Use It (3 Steps)

### Step 1: Load Model
1. Go to **Fine-Tuning** tab
2. Click **"ğŸ”„ Load"** on Codellama-7b
3. Wait for it to load

**You'll see**:
- First time: "ğŸ†• **Base Mode:** Next training will be v1"
- After training: "ğŸ”„ **Incremental Mode:** Loaded v1_TIMESTAMP"

### Step 2: Train
1. Enter meeting notes
2. Check "ğŸš€ Unlimited Mode"
3. Click "Preview Dataset"
4. Click "ğŸš€ Start Fine-Tuning"

**Logs will show**:
```
[INCREMENTAL] Training v1_20251105_140000 (builds on base model)
```

### Step 3: Give Feedback & Train Again
1. Generate some artifacts (ERD, API, prototype)
2. Click "ğŸ‘ Good" or "ğŸ‘ Needs Improvement"
3. Go back to Fine-Tuning tab
4. Click "Preview Dataset" (see your feedback added!)
5. Click "ğŸš€ Start Fine-Tuning" again

**Logs will show**:
```
[INCREMENTAL] Loading previous fine-tuned model: v1_20251105_140000
[INCREMENTAL] âœ… Successfully loaded fine-tuned model!
[INCREMENTAL] Training v2_20251105_153000 (builds on v1_20251105_140000)
```

**That's it!** v2 now contains all improvements from v1 + your new feedback! ğŸš€

---

## Bonus: Rollback Feature

### What if v3 was trained badly?

1. Go to Fine-Tuning tab
2. Expand **"View all versions / Rollback"**
3. Click **"Load"** next to v2 (the good one)
4. Train again â†’ Creates v4 (builds on v2, skips bad v3)

**You're back on track!** ğŸ¯

---

## Key Features

1. **âœ… Automatic** - System detects and loads latest version
2. **âœ… Cumulative** - Each run builds on previous improvements
3. **âœ… Versioned** - Every training creates a new version (v1, v2, v3...)
4. **âœ… Rollback** - Load any previous version if needed
5. **âœ… Safe** - Never overwrites existing versions

---

## What Changed in the Code?

**You don't need to know this, but if you're curious**:

1. **`load_model()`** now checks for existing fine-tuned versions and loads the latest
2. **Version names** are auto-generated: `vN_YYYYMMDD_HHMMSS`
3. **Training saves** to version-specific folders: `finetuned_models/codellama-7b/v1_TIMESTAMP/`
4. **UI shows** current version status and lists all available versions
5. **Rollback** is a simple button click to load any previous version

---

## Logs To Watch For

### When Loading Model (First Time):
```
[INFO] Loading base model (no previous fine-tuning found)
[DEBUG] Model loaded successfully (base model)!
```

### When Loading Model (After Training):
```
[INCREMENTAL] Loading previous fine-tuned model: v2_20251105_153000
[INCREMENTAL] âœ… Successfully loaded fine-tuned model!
[INCREMENTAL] Next training will build on: v2_20251105_153000
[DEBUG] Model loaded successfully (incremental from v2_20251105_153000)!
```

### When Training:
```
[INCREMENTAL] Training v3_20251105_160000 (builds on v2_20251105_153000)
```

---

## Expected Workflow

```
Day 1:
  Load base â†’ Train â†’ v1 created
  âœ… Model learns your codebase patterns

Day 2:
  Generate artifacts â†’ Give feedback (5 entries)
  Load v1 â†’ Train â†’ v2 created
  âœ… Model improves with v1 knowledge + new feedback

Day 3:
  Generate more â†’ Give more feedback (8 entries total)
  Load v2 â†’ Train â†’ v3 created
  âœ… Model is now significantly better (v1 + v2 + new feedback)

Day 4:
  Notice v3 is worse â†’ Rollback to v2
  Load v2 â†’ Add feedback (12 entries total)
  Load v2 â†’ Train â†’ v4 created
  âœ… Back on track with good v2 + corrections
```

---

## File Structure

```
finetuned_models/
â””â”€â”€ codellama-7b/
    â”œâ”€â”€ v1_20251105_140000/     â† First training
    â”‚   â”œâ”€â”€ adapter_config.json
    â”‚   â””â”€â”€ adapter_model.bin    (100-200MB)
    â”œâ”€â”€ v2_20251105_153000/     â† Builds on v1
    â”‚   â”œâ”€â”€ adapter_config.json
    â”‚   â””â”€â”€ adapter_model.bin
    â””â”€â”€ v3_20251105_160000/     â† Builds on v2
        â”œâ”€â”€ adapter_config.json
        â””â”€â”€ adapter_model.bin
```

**Note**: Each version is only ~100-200MB (LoRA adapters, not full models).

---

## Quick Troubleshooting

### "Still showing Base Mode after training"
â†’ **Restart the app!** The old code is still running.

### "Can't see version list"
â†’ **Train at least once first.** Versions appear after first training.

### "Training takes forever"
â†’ **Normal!** Training takes 30-60 minutes. But quality improves faster now because you start from a better baseline each time.

### "Want to start completely fresh"
â†’ Delete `finetuned_models/codellama-7b/` folder, then reload the model. It will start from base again.

---

## ğŸ‰ You're Done!

**Restart the app and try it out!**

1. **Restart**: `Ctrl+C` â†’ `python launch.py`
2. **Load model**: Fine-Tuning tab â†’ Click "Load"
3. **Check status**: Should show "Base Mode" or "Incremental Mode"
4. **Train once**: Create v1
5. **Train again**: Creates v2 (builds on v1!)

**Your model will now continuously improve with each iteration!** ğŸš€

---

**For more details**: Read `INCREMENTAL_TRAINING_GUIDE.md` (complete guide with examples and FAQ).

