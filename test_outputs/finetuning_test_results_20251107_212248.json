{
  "ollama_adaptive_loop": {
    "import": "PASS",
    "initialization": "PASS",
    "feedback_recording": "FAIL: Object of type FeedbackType is not JSON serializable"
  },
  "huggingface_pipeline": {
    "import": "PASS",
    "initialization": "PASS",
    "available_models": [
      "codellama-7b",
      "llama3-8b",
      "mistral-7b",
      "deepseek-coder-6.7b",
      "mermaid-mistral-7b"
    ],
    "environment": {
      "status": "PASS",
      "details": {
        "os": "Windows",
        "has_cuda": true,
        "has_bitsandbytes": false,
        "ready": false,
        "message": "\u26a0\ufe0f Only 12.0GB VRAM detected. Recommend 12GB+ for 7B models with LoRA. Training may be slower or fail with OOM errors. Consider: (1) Reduce batch size to 1, (2) Use 4-bit quantization, or (3) Switch to cloud provider.",
        "vram_gb": 11.99365234375
      }
    },
    "system_info": {
      "status": "PASS",
      "details": {
        "ram_gb": 31.692127227783203,
        "cpu_count": 32,
        "disk_free_gb": 502.9300994873047,
        "python_version": "3.8+",
        "cuda_available": true,
        "bitsandbytes_available": false,
        "environment_ready": false
      }
    },
    "dataset_builder": {
      "status": "PASS",
      "examples_generated": 486,
      "report": {
        "total": 486,
        "feedback": 0,
        "files": 49
      }
    },
    "training_config": "PASS",
    "lr_validation": "SKIP: No model loaded",
    "checkpoint_management": {
      "status": "PASS",
      "existing_models": 0
    },
    "incremental_training": {
      "status": "PASS",
      "downloaded_models": 2
    }
  },
  "timestamp": "2025-11-07T21:22:35.951665",
  "summary": {
    "ollama_adaptive_loop": "FAIL",
    "huggingface_pipeline": "PASS",
    "overall": "FAIL"
  }
}