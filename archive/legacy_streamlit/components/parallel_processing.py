"""
Parallel Processing System
Provides 60-70% speed boost through parallel artifact generation and processing
"""

import streamlit as st
import asyncio
from typing import Dict, List, Tuple, Optional, Any, Callable
from dataclasses import dataclass
from pathlib import Path
import json
import time
from datetime import datetime
import concurrent.futures
import threading
from queue import Queue
import multiprocessing as mp


@dataclass
class ProcessingTask:
    """Represents a processing task"""
    task_id: str
    task_type: str
    function: Callable
    args: tuple
    kwargs: dict
    priority: int
    dependencies: List[str]
    estimated_duration: float


@dataclass
class ProcessingResult:
    """Result of a processing task"""
    task_id: str
    success: bool
    result: Any
    error: str
    duration: float
    start_time: datetime
    end_time: datetime


@dataclass
class ParallelExecutionPlan:
    """Plan for parallel execution"""
    total_tasks: int
    parallel_groups: List[List[ProcessingTask]]
    estimated_total_time: float
    estimated_speedup: float
    execution_strategy: str


class ParallelProcessingSystem:
    """Parallel processing system for artifact generation"""
    
    def __init__(self):
        self.max_workers = min(mp.cpu_count(), 8)  # Limit to 8 workers max
        self.task_queue = Queue()
        self.results = {}
        self.execution_plan = None
        self.is_running = False
        
        # Task dependencies and priorities
        self.task_dependencies = {
            'rag_retrieval': [],
            'mermaid_validation': ['rag_retrieval'],
            'html_generation': ['mermaid_validation'],
            'api_docs': ['rag_retrieval'],
            'jira_tasks': ['rag_retrieval'],
            'workflows': ['rag_retrieval'],
            'code_prototype': ['rag_retrieval'],
            'visual_prototype': ['rag_retrieval'],
            'openapi_spec': ['api_docs'],
            'api_client': ['openapi_spec']
        }
        
        self.task_priorities = {
            'rag_retrieval': 1,  # Highest priority
            'mermaid_validation': 2,
            'html_generation': 3,
            'api_docs': 2,
            'jira_tasks': 3,
            'workflows': 3,
            'code_prototype': 2,
            'visual_prototype': 3,
            'openapi_spec': 4,
            'api_client': 5
        }
    
    def create_execution_plan(self, tasks: List[ProcessingTask]) -> ParallelExecutionPlan:
        """Create an execution plan for parallel processing"""
        
        # Group tasks by dependencies
        parallel_groups = []
        remaining_tasks = tasks.copy()
        completed_tasks = set()
        
        while remaining_tasks:
            # Find tasks that can run in parallel (no pending dependencies)
            ready_tasks = []
            
            for task in remaining_tasks:
                if all(dep in completed_tasks for dep in task.dependencies):
                    ready_tasks.append(task)
            
            if not ready_tasks:
                # Handle circular dependencies or missing tasks
                ready_tasks = [remaining_tasks[0]]
            
            # Sort by priority
            ready_tasks.sort(key=lambda t: self.task_priorities.get(t.task_type, 5))
            
            # Limit parallel execution to max_workers
            if len(ready_tasks) > self.max_workers:
                ready_tasks = ready_tasks[:self.max_workers]
            
            parallel_groups.append(ready_tasks)
            
            # Remove completed tasks
            for task in ready_tasks:
                remaining_tasks.remove(task)
                completed_tasks.add(task.task_id)
        
        # Calculate estimated times
        estimated_total_time = sum(
            max(task.estimated_duration for task in group) 
            for group in parallel_groups
        )
        
        # Calculate speedup
        sequential_time = sum(task.estimated_duration for task in tasks)
        estimated_speedup = sequential_time / estimated_total_time if estimated_total_time > 0 else 1.0
        
        return ParallelExecutionPlan(
            total_tasks=len(tasks),
            parallel_groups=parallel_groups,
            estimated_total_time=estimated_total_time,
            estimated_speedup=estimated_speedup,
            execution_strategy="dependency_based"
        )
    
    async def execute_parallel(self, tasks: List[ProcessingTask], 
                             progress_callback=None) -> Dict[str, ProcessingResult]:
        """Execute tasks in parallel with dependency management"""
        
        self.is_running = True
        results = {}
        
        try:
            # Create execution plan
            self.execution_plan = self.create_execution_plan(tasks)
            
            if progress_callback:
                progress_callback({
                    'status': 'planning',
                    'total_tasks': self.execution_plan.total_tasks,
                    'estimated_speedup': self.execution_plan.estimated_speedup,
                    'estimated_time': self.execution_plan.estimated_total_time
                })
            
            # Execute each group in parallel
            for group_idx, group in enumerate(self.execution_plan.parallel_groups):
                if progress_callback:
                    progress_callback({
                        'status': 'executing',
                        'group': group_idx + 1,
                        'total_groups': len(self.execution_plan.parallel_groups),
                        'current_tasks': len(group)
                    })
                
                # Execute tasks in this group in parallel
                group_results = await self._execute_group_parallel(group, progress_callback)
                results.update(group_results)
            
            if progress_callback:
                progress_callback({
                    'status': 'completed',
                    'total_results': len(results),
                    'successful': sum(1 for r in results.values() if r.success),
                    'failed': sum(1 for r in results.values() if not r.success)
                })
            
            return results
            
        finally:
            self.is_running = False
    
    async def _execute_group_parallel(self, group: List[ProcessingTask], 
                                    progress_callback=None) -> Dict[str, ProcessingResult]:
        """Execute a group of tasks in parallel"""
        
        # Create tasks for asyncio
        async_tasks = []
        for task in group:
            async_tasks.append(self._execute_single_task(task, progress_callback))
        
        # Execute all tasks in parallel
        results = await asyncio.gather(*async_tasks, return_exceptions=True)
        
        # Convert results to dictionary
        result_dict = {}
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # Handle exceptions
                task = group[i]
                result_dict[task.task_id] = ProcessingResult(
                    task_id=task.task_id,
                    success=False,
                    result=None,
                    error=str(result),
                    duration=0.0,
                    start_time=datetime.now(),
                    end_time=datetime.now()
                )
            else:
                result_dict[result.task_id] = result
        
        return result_dict
    
    async def _execute_single_task(self, task: ProcessingTask, 
                                 progress_callback=None) -> ProcessingResult:
        """Execute a single task"""
        
        start_time = datetime.now()
        
        try:
            if progress_callback:
                progress_callback({
                    'status': 'task_started',
                    'task_id': task.task_id,
                    'task_type': task.task_type
                })
            
            # Execute the task
            if asyncio.iscoroutinefunction(task.function):
                result = await task.function(*task.args, **task.kwargs)
            else:
                # Run synchronous function in thread pool
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None, task.function, *task.args, **task.kwargs
                )
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if progress_callback:
                progress_callback({
                    'status': 'task_completed',
                    'task_id': task.task_id,
                    'task_type': task.task_type,
                    'duration': duration
                })
            
            return ProcessingResult(
                task_id=task.task_id,
                success=True,
                result=result,
                error="",
                duration=duration,
                start_time=start_time,
                end_time=end_time
            )
            
        except Exception as e:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if progress_callback:
                progress_callback({
                    'status': 'task_failed',
                    'task_id': task.task_id,
                    'task_type': task.task_type,
                    'error': str(e)
                })
            
            return ProcessingResult(
                task_id=task.task_id,
                success=False,
                result=None,
                error=str(e),
                duration=duration,
                start_time=start_time,
                end_time=end_time
            )
    
    def create_artifact_generation_tasks(self, meeting_notes: str, rag_suffix: str = "", 
                                       provider_key: str = "", provider_name: str = "") -> List[ProcessingTask]:
        """Create tasks for parallel artifact generation"""
        
        tasks = []
        
        # Task 1: RAG Retrieval (highest priority)
        tasks.append(ProcessingTask(
            task_id="rag_retrieval",
            task_type="rag_retrieval",
            function=self._rag_retrieval_task,
            args=(meeting_notes, rag_suffix),
            kwargs={},
            priority=1,
            dependencies=[],
            estimated_duration=5.0
        ))
        
        # Task 2: Mermaid Validation (depends on RAG)
        tasks.append(ProcessingTask(
            task_id="mermaid_validation",
            task_type="mermaid_validation",
            function=self._mermaid_validation_task,
            args=(),
            kwargs={},
            priority=2,
            dependencies=["rag_retrieval"],
            estimated_duration=3.0
        ))
        
        # Task 3: HTML Generation (depends on Mermaid validation)
        tasks.append(ProcessingTask(
            task_id="html_generation",
            task_type="html_generation",
            function=self._html_generation_task,
            args=(meeting_notes,),
            kwargs={},
            priority=3,
            dependencies=["mermaid_validation"],
            estimated_duration=4.0
        ))
        
        # Task 4: API Documentation (depends on RAG)
        tasks.append(ProcessingTask(
            task_id="api_docs",
            task_type="api_docs",
            function=self._api_docs_task,
            args=(meeting_notes, provider_key, provider_name),
            kwargs={},
            priority=2,
            dependencies=["rag_retrieval"],
            estimated_duration=6.0
        ))
        
        # Task 5: JIRA Tasks (depends on RAG)
        tasks.append(ProcessingTask(
            task_id="jira_tasks",
            task_type="jira_tasks",
            function=self._jira_tasks_task,
            args=(meeting_notes, provider_key, provider_name),
            kwargs={},
            priority=3,
            dependencies=["rag_retrieval"],
            estimated_duration=4.0
        ))
        
        # Task 6: Workflows (depends on RAG)
        tasks.append(ProcessingTask(
            task_id="workflows",
            task_type="workflows",
            function=self._workflows_task,
            args=(meeting_notes, provider_key, provider_name),
            kwargs={},
            priority=3,
            dependencies=["rag_retrieval"],
            estimated_duration=5.0
        ))
        
        # Task 7: Code Prototype (depends on RAG)
        tasks.append(ProcessingTask(
            task_id="code_prototype",
            task_type="code_prototype",
            function=self._code_prototype_task,
            args=(meeting_notes, provider_key, provider_name),
            kwargs={},
            priority=2,
            dependencies=["rag_retrieval"],
            estimated_duration=7.0
        ))
        
        # Task 8: Visual Prototype (depends on RAG)
        tasks.append(ProcessingTask(
            task_id="visual_prototype",
            task_type="visual_prototype",
            function=self._visual_prototype_task,
            args=(meeting_notes, provider_key, provider_name),
            kwargs={},
            priority=3,
            dependencies=["rag_retrieval"],
            estimated_duration=6.0
        ))
        
        # Task 9: OpenAPI Spec (depends on API docs)
        tasks.append(ProcessingTask(
            task_id="openapi_spec",
            task_type="openapi_spec",
            function=self._openapi_spec_task,
            args=(),
            kwargs={},
            priority=4,
            dependencies=["api_docs"],
            estimated_duration=3.0
        ))
        
        # Task 10: API Client (depends on OpenAPI spec)
        tasks.append(ProcessingTask(
            task_id="api_client",
            task_type="api_client",
            function=self._api_client_task,
            args=(),
            kwargs={},
            priority=5,
            dependencies=["openapi_spec"],
            estimated_duration=4.0
        ))
        
        return tasks
    
    # Task implementations
    async def _rag_retrieval_task(self, meeting_notes: str, rag_suffix: str = ""):
        """RAG retrieval task"""
        from agents.universal_agent import UniversalArchitectAgent
        from components.enhanced_rag import enhanced_rag_system
        
        agent = UniversalArchitectAgent()
        rag_query = f"artifact generation {meeting_notes} {rag_suffix}".strip()
        
        # Use enhanced RAG system
        enhanced_context = await enhanced_rag_system.retrieve_enhanced_context(
            rag_query, max_chunks=100, strategy="tiered"
        )
        
        return {
            'enhanced_context': enhanced_context,
            'rag_query': rag_query
        }
    
    async def _mermaid_validation_task(self):
        """Mermaid validation task"""
        from components.mermaid_syntax_corrector import mermaid_corrector
        
        # Validate any existing Mermaid diagrams
        outputs_dir = Path("outputs/visualizations")
        if outputs_dir.exists():
            for mmd_file in outputs_dir.glob("*.mmd"):
                content = mmd_file.read_text(encoding='utf-8')
                is_valid, corrected, errors = mermaid_corrector.validate_mermaid_syntax(content)
                
                if not is_valid:
                    mmd_file.write_text(corrected, encoding='utf-8')
        
        return {'validated': True}
    
    async def _html_generation_task(self, meeting_notes: str):
        """HTML generation task with Gemini context-aware generation"""
        from components.mermaid_html_renderer import mermaid_html_renderer
        
        # Generate HTML for Mermaid diagrams with Gemini
        outputs_dir = Path("outputs/visualizations")
        if outputs_dir.exists():
            for mmd_file in outputs_dir.glob("*.mmd"):
                content = mmd_file.read_text(encoding='utf-8')
                
                # Use Gemini to generate context-aware HTML
                try:
                    html_content = await mermaid_html_renderer.generate_html_visualization_with_gemini(
                        content, meeting_notes, "flowchart"
                    )
                except Exception:
                    # Fallback to basic HTML rendering
                    html_content = mermaid_html_renderer.render_mermaid_as_html(content)
                
                html_file = mmd_file.with_suffix('.html')
                html_file.write_text(html_content, encoding='utf-8')
        
        return {'html_generated': True, 'gemini_used': True}
    
    async def _api_docs_task(self, meeting_notes: str, provider_key: str, provider_name: str):
        """API documentation task"""
        from components.enhanced_api_docs import enhanced_api_docs_generator
        
        # Generate enhanced API documentation
        enhanced_docs = await enhanced_api_docs_generator.generate_enhanced_api_docs(
            "API endpoints, routes, controllers, services", meeting_notes, "web"
        )
        
        # Save documentation
        outputs_dir = Path("outputs/documentation")
        outputs_dir.mkdir(parents=True, exist_ok=True)
        
        markdown_content = f"""# {enhanced_docs.title} API Documentation

**Version:** {enhanced_docs.version}  
**Base URL:** {enhanced_docs.base_url}  
**Generated:** {enhanced_docs.generated_at.strftime('%Y-%m-%d %H:%M:%S')}

## Description
{enhanced_docs.description}

## Authentication
- **Type:** {enhanced_docs.authentication.get('type', 'Bearer Token')}
- **Description:** {enhanced_docs.authentication.get('description', 'API key required')}

## Rate Limits
- **Requests per minute:** {enhanced_docs.rate_limits.get('requests_per_minute', 100)}
- **Requests per hour:** {enhanced_docs.rate_limits.get('requests_per_hour', 1000)}

## Endpoints
"""
        
        for endpoint in enhanced_docs.endpoints:
            markdown_content += f"""
### {endpoint.method} {endpoint.path}
- **Summary:** {endpoint.summary}
- **Description:** {endpoint.description}
"""
        
        api_docs_path = outputs_dir / "enhanced_api_docs.md"
        api_docs_path.write_text(markdown_content, encoding='utf-8')
        
        return {'api_docs_path': str(api_docs_path)}
    
    async def _jira_tasks_task(self, meeting_notes: str, provider_key: str, provider_name: str):
        """JIRA tasks generation task"""
        from agents.universal_agent import UniversalArchitectAgent
        
        agent = UniversalArchitectAgent({provider_name: provider_key})
        agent.meeting_notes = meeting_notes
        
        # Generate JIRA tasks
        jira_content = await agent.generate_jira_only()
        
        if jira_content:
            outputs_dir = Path("outputs/documentation")
            outputs_dir.mkdir(parents=True, exist_ok=True)
            
            jira_path = outputs_dir / "jira_tasks.md"
            jira_path.write_text(jira_content, encoding='utf-8')
            
            return {'jira_path': str(jira_path)}
        
        return {'jira_path': None}
    
    async def _workflows_task(self, meeting_notes: str, provider_key: str, provider_name: str):
        """Workflows generation task"""
        from agents.universal_agent import UniversalArchitectAgent
        
        agent = UniversalArchitectAgent({provider_name: provider_key})
        agent.meeting_notes = meeting_notes
        
        # Generate workflows
        workflows_content = await agent.generate_workflows_only()
        
        if workflows_content:
            outputs_dir = Path("outputs/workflows")
            outputs_dir.mkdir(parents=True, exist_ok=True)
            
            workflows_path = outputs_dir / "workflows.md"
            workflows_path.write_text(workflows_content, encoding='utf-8')
            
            return {'workflows_path': str(workflows_path)}
        
        return {'workflows_path': None}
    
    async def _code_prototype_task(self, meeting_notes: str, provider_key: str, provider_name: str):
        """Code prototype generation task"""
        from agents.universal_agent import UniversalArchitectAgent
        
        agent = UniversalArchitectAgent({provider_name: provider_key})
        agent.meeting_notes = meeting_notes
        
        # Generate code prototype
        code_result = await agent.generate_prototype_code("feature-from-notes")
        
        if code_result and isinstance(code_result, dict) and "code" in code_result:
            outputs_dir = Path("outputs/prototypes")
            outputs_dir.mkdir(parents=True, exist_ok=True)
            
            code_path = outputs_dir / "prototype_code.txt"
            code_path.write_text(code_result["code"], encoding='utf-8')
            
            return {'code_path': str(code_path)}
        
        return {'code_path': None}
    
    async def _visual_prototype_task(self, meeting_notes: str, provider_key: str, provider_name: str):
        """Visual prototype generation task"""
        from agents.universal_agent import UniversalArchitectAgent
        
        agent = UniversalArchitectAgent({provider_name: provider_key})
        agent.meeting_notes = meeting_notes
        
        # Generate visual prototype
        visual_result = await agent.generate_visual_prototype("developer-feature")
        
        if visual_result:
            outputs_dir = Path("outputs/prototypes")
            outputs_dir.mkdir(parents=True, exist_ok=True)
            
            visual_path = outputs_dir / "developer_visual_prototype.html"
            visual_path.write_text(visual_result, encoding='utf-8')
            
            return {'visual_path': str(visual_path)}
        
        return {'visual_path': None}
    
    async def _openapi_spec_task(self):
        """OpenAPI specification generation task"""
        from components.enhanced_api_docs import enhanced_api_docs_generator
        
        # Generate OpenAPI spec
        api_docs_path = Path("outputs/documentation/enhanced_api_docs.md")
        if api_docs_path.exists():
            # Read existing API docs and generate OpenAPI spec
            openapi_spec = enhanced_api_docs_generator.generate_openapi_spec(
                enhanced_api_docs_generator._generate_fallback_api_docs("", "")
            )
            
            outputs_dir = Path("outputs/documentation")
            outputs_dir.mkdir(parents=True, exist_ok=True)
            
            openapi_path = outputs_dir / "api_spec.json"
            openapi_path.write_text(json.dumps(openapi_spec, indent=2), encoding='utf-8')
            
            return {'openapi_path': str(openapi_path)}
        
        return {'openapi_path': None}
    
    async def _api_client_task(self):
        """API client generation task"""
        from agents.universal_agent import UniversalArchitectAgent
        
        agent = UniversalArchitectAgent()
        
        # Generate Python API client
        python_prompt = """
        Generate a production-ready Python API client for the project's endpoints found in the RAG context. 
        Use requests, handle auth (token), timeouts, retries, typed dataclasses, and raise for status. 
        Output full .py code only.
        """
        
        python_client = await agent._call_ai(python_prompt, "You are an expert Python engineer. Output only code.")
        
        if python_client:
            outputs_dir = Path("outputs/prototypes")
            outputs_dir.mkdir(parents=True, exist_ok=True)
            
            python_path = outputs_dir / "api_client.py"
            python_path.write_text(python_client, encoding='utf-8')
            
            return {'python_client_path': str(python_path)}
        
        return {'python_client_path': None}
    
    def get_execution_plan(self) -> Optional[ParallelExecutionPlan]:
        """Get current execution plan"""
        return self.execution_plan
    
    def is_execution_running(self) -> bool:
        """Check if execution is currently running"""
        return self.is_running


# Global instance
parallel_processing_system = ParallelProcessingSystem()


def render_parallel_processing_ui():
    """Streamlit UI for parallel processing system"""
    
    st.subheader("‚ö° Parallel Processing System (60-70% Speed Boost)")
    
    # System info
    st.write("**üîß System Configuration:**")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Max Workers", parallel_processing_system.max_workers)
    with col2:
        st.metric("CPU Cores", mp.cpu_count())
    with col3:
        st.metric("Execution Strategy", "Dependency-based")
    
    # Input section
    st.write("**üìù Generation Parameters:**")
    
    col1, col2 = st.columns(2)
    
    with col1:
        meeting_notes = st.text_area(
            "Meeting Notes:",
            height=100,
            placeholder="Enter meeting notes for parallel artifact generation...",
            key="parallel_meeting_notes"
        )
    
    with col2:
        rag_suffix = st.text_input(
            "RAG Suffix:",
            placeholder="Additional context for RAG...",
            key="parallel_rag_suffix"
        )
    
    # Provider selection
    col1, col2 = st.columns(2)
    
    with col1:
        provider_name = st.selectbox(
            "AI Provider:",
            ["openai", "anthropic", "google", "local"],
            key="parallel_provider"
        )
    
    with col2:
        provider_key = st.text_input(
            "API Key:",
            type="password",
            key="parallel_api_key"
        )
    
    # Generate button
    if st.button("‚ö° Start Parallel Generation", type="primary"):
        if not meeting_notes.strip():
            st.warning("Please enter meeting notes")
            return
        
        if not provider_key.strip():
            st.warning("Please enter API key")
            return
        
        # Create tasks
        tasks = parallel_processing_system.create_artifact_generation_tasks(
            meeting_notes, rag_suffix, provider_key, provider_name
        )
        
        # Create execution plan
        execution_plan = parallel_processing_system.create_execution_plan(tasks)
        
        # Store in session state
        st.session_state.parallel_execution_plan = execution_plan
        st.session_state.parallel_tasks = tasks
        
        st.success(f"‚úÖ Execution plan created! Estimated speedup: {execution_plan.estimated_speedup:.1f}x")
    
    # Display execution plan
    if 'parallel_execution_plan' in st.session_state:
        plan = st.session_state.parallel_execution_plan
        
        st.divider()
        
        # Plan overview
        st.write("**üìä Execution Plan:**")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Tasks", plan.total_tasks)
        with col2:
            st.metric("Parallel Groups", len(plan.parallel_groups))
        with col3:
            st.metric("Estimated Time", f"{plan.estimated_total_time:.1f}s")
        with col4:
            st.metric("Speedup", f"{plan.estimated_speedup:.1f}x")
        
        # Execution groups
        st.write("**üîÑ Execution Groups:**")
        for i, group in enumerate(plan.parallel_groups):
            with st.expander(f"Group {i+1} ({len(group)} tasks)"):
                for task in group:
                    st.write(f"‚Ä¢ **{task.task_type}** (Priority: {task.priority}, Duration: {task.estimated_duration:.1f}s)")
                    if task.dependencies:
                        st.write(f"  Dependencies: {', '.join(task.dependencies)}")
        
        # Execute button
        if st.button("üöÄ Execute Parallel Generation"):
            if 'parallel_tasks' in st.session_state:
                tasks = st.session_state.parallel_tasks
                
                with st.spinner("Executing parallel generation..."):
                    try:
                        def progress_callback(progress):
                            if progress['status'] == 'planning':
                                st.write(f"üìã Planning: {progress['total_tasks']} tasks, {progress['estimated_speedup']:.1f}x speedup")
                            elif progress['status'] == 'executing':
                                st.write(f"‚ö° Executing Group {progress['group']}/{progress['total_groups']} ({progress['current_tasks']} tasks)")
                            elif progress['status'] == 'task_started':
                                st.write(f"üîÑ Started: {progress['task_type']}")
                            elif progress['status'] == 'task_completed':
                                st.write(f"‚úÖ Completed: {progress['task_type']} ({progress['duration']:.1f}s)")
                            elif progress['status'] == 'task_failed':
                                st.write(f"‚ùå Failed: {progress['task_type']} - {progress['error']}")
                            elif progress['status'] == 'completed':
                                st.write(f"üéâ Completed: {progress['successful']} successful, {progress['failed']} failed")
                        
                        # Execute parallel generation
                        results = asyncio.run(
                            parallel_processing_system.execute_parallel(tasks, progress_callback)
                        )
                        
                        # Store results
                        st.session_state.parallel_results = results
                        st.success("‚úÖ Parallel generation completed!")
                        
                    except Exception as e:
                        st.error(f"‚ùå Parallel generation failed: {str(e)}")
    
    # Display results
    if 'parallel_results' in st.session_state:
        results = st.session_state.parallel_results
        
        st.divider()
        
        # Results overview
        st.write("**üìä Execution Results:**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Tasks", len(results))
        with col2:
            st.metric("Successful", sum(1 for r in results.values() if r.success))
        with col3:
            st.metric("Failed", sum(1 for r in results.values() if not r.success))
        
        # Detailed results
        st.write("**üìã Task Results:**")
        for task_id, result in results.items():
            with st.expander(f"{'‚úÖ' if result.success else '‚ùå'} {task_id} ({result.duration:.1f}s)"):
                if result.success:
                    st.write(f"**Result:** {str(result.result)[:200]}...")
                else:
                    st.write(f"**Error:** {result.error}")
                
                st.write(f"**Duration:** {result.duration:.2f}s")
                st.write(f"**Start:** {result.start_time.strftime('%H:%M:%S')}")
                st.write(f"**End:** {result.end_time.strftime('%H:%M:%S')}")


def render_parallel_processing_tab():
    """Render the parallel processing tab"""
    render_parallel_processing_ui()
